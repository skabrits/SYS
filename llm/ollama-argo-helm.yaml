# apiVersion: argoproj.io/v1alpha1
# kind: Application
# metadata:
#   name: ollama
#   namespace: argocd
#   annotations:
#     argocd.argoproj.io/sync-wave: "10"
#   finalizers:
#     - resources-finalizer.argocd.argoproj.io
# spec:
#   syncPolicy:
#     syncOptions:
#       - CreateNamespace=true
#     automated:
#       prune: true
#       selfHeal: true
#       allowEmpty: true
#   project: default
#   source:
#     chart: ollama 
#     repoURL: https://otwld.github.io/ollama-helm/
#     targetRevision: "*"
#     helm:
#       releaseName: llama3
#       parameters:
#       - name: "replicaCount"
#         value: "1"
#       - name: "ollama.gpu.enabled"
#         value: "true"
#       - name: "ollama.models.pull[0]"
#         value: "qwen2.5vl:7b"
#       - name: "ollama.models.pull[1]"
#         value: "qwen2.5vl:32b"
#       - name: "persistentVolume.enabled"
#         value: "true"
#       - name: "persistentVolume.size"
#         value: "80Gi"
#       - name: "persistentVolume.storageClass"
#         value: "longhorn"
#       - name: "runtimeClassName"
#         value: "nvidia"
#       - name: "service.type"
#         value: "LoadBalancer"
#       - name: "service.annotations.metallb\\.universe\\.tf/loadBalancerIPs"
#         value: "192.168.0.154"
#       - name: "resources.requests.cpu"
#         value: "1m"
#       - name: "resources.requests.memory"
#         value: "1Ki"
#   destination:
#     server: "https://kubernetes.default.svc"
#     namespace: ollama
